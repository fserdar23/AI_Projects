{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ef978a-72b9-4c55-ba54-b4cf9707520e",
   "metadata": {},
   "source": [
    "# Cleaned vs Dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede1d2f-b4ec-4c54-94b8-3ecd23adb259",
   "metadata": {},
   "source": [
    "<img src=\"https://www.kaggle.com/competitions/15282/images/header\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244b42f-9418-4fc4-98b7-8930a7458d41",
   "metadata": {},
   "source": [
    "Bu projemizde tabakların temiz mi yoksa kirli mi oldugunu bulan bir model gelistirecegiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b5e74-724f-4f7b-a64d-879d26618942",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/competitions/platesv2/data\">Dataya Buradan Erişebilirsiniz</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07163a5e-89c5-4035-a695-eb40df0642a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b133e0dc-e4b6-4623-9893-16b0213345f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"train/\"\n",
    "labels=os.listdir(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a0ba1c-b77a-4f47-bf73-84288ec988a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleaned', 'dirty']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041ca6d0-23b9-4d4c-abcb-0beb0758720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dosyadan Resim okuma\n",
    "img_list=[]\n",
    "label_list=[]\n",
    "for label in labels:\n",
    "    for img_file in os.listdir(img_path+label):       #Klasörün içindeki alt klasörleri söyler\n",
    "        img_list.append(img_path+label+\"/\"+img_file)  #apppend dizinin sonuna eklme yapıyor\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d07c67d-f065-4872-8515-5233180215fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"img\":img_list,\"label\":label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270fd2b6-da29-45dd-bc70-26fc96f1f019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/cleaned/0000.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/cleaned/0001.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/cleaned/0002.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/cleaned/0003.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/cleaned/0004.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      img    label\n",
       "0  train/cleaned/0000.jpg  cleaned\n",
       "1  train/cleaned/0001.jpg  cleaned\n",
       "2  train/cleaned/0002.jpg  cleaned\n",
       "3  train/cleaned/0003.jpg  cleaned\n",
       "4  train/cleaned/0004.jpg  cleaned"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256dbee9-eeea-4c82-80ee-24e399a4193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'cleaned':0, 'dirty':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303e5377-5d07-4fe0-b461-395327f86722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"encode_label\"]=df[\"label\"].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90a3a21-2192-471c-8f98-2b8da42f5c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>encode_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/cleaned/0000.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/cleaned/0001.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/cleaned/0002.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/cleaned/0003.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/cleaned/0004.jpg</td>\n",
       "      <td>cleaned</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>train/dirty/0596.jpg</td>\n",
       "      <td>dirty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>train/dirty/0703.jpg</td>\n",
       "      <td>dirty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>train/dirty/0707.jpg</td>\n",
       "      <td>dirty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>train/dirty/0710.jpg</td>\n",
       "      <td>dirty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>train/dirty/0730.jpg</td>\n",
       "      <td>dirty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        img    label  encode_label\n",
       "0    train/cleaned/0000.jpg  cleaned             0\n",
       "1    train/cleaned/0001.jpg  cleaned             0\n",
       "2    train/cleaned/0002.jpg  cleaned             0\n",
       "3    train/cleaned/0003.jpg  cleaned             0\n",
       "4    train/cleaned/0004.jpg  cleaned             0\n",
       "..                      ...      ...           ...\n",
       "137    train/dirty/0596.jpg    dirty             1\n",
       "138    train/dirty/0703.jpg    dirty             1\n",
       "139    train/dirty/0707.jpg    dirty             1\n",
       "140    train/dirty/0710.jpg    dirty             1\n",
       "141    train/dirty/0730.jpg    dirty             1\n",
       "\n",
       "[142 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd346c1-1fe1-478e-88a5-a475ae9009c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "        img=cv2.imread(img)\n",
    "        img=cv2.resize(img,(70,70))\n",
    "        img=img/255\n",
    "        x.append(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b07d2c-3c2d-4db2-96ec-9051f63f7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for img in df[\"img\"]:\n",
    "    process_image(img)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbf672c-50d6-4bda-b149-a246450c7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58e2ff43-3db9-43c0-bd89-f5db3a7b7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"encode_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b06438a-677f-4f3e-bbfd-e80ff0772ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38b6e8cb-d9a3-4649-867e-26351b29741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff2a2b22-182c-446a-a8fd-6087a1ee8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D,Dense,Flatten, Input, MaxPooling2D, Dropout, BatchNormalization, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "359692eb-7b2b-456c-8a99-06a030639220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Input(shape=(70,70,3)))\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25)) # Burada 'rate' parametresini belirtiyoruz\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5d9101-3914-4599-9001-216424eb0092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.5869 - loss: 3.5911 - val_accuracy: 0.4828 - val_loss: 1.0552\n",
      "Epoch 2/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8298 - loss: 4.1016 - val_accuracy: 0.6552 - val_loss: 0.5760\n",
      "Epoch 3/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8884 - loss: 1.9631 - val_accuracy: 0.5517 - val_loss: 1.5301\n",
      "Epoch 4/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9352 - loss: 1.0585 - val_accuracy: 0.5517 - val_loss: 1.0224\n",
      "Epoch 5/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8665 - loss: 1.9331 - val_accuracy: 0.7931 - val_loss: 0.5508\n",
      "Epoch 6/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9706 - loss: 0.3567 - val_accuracy: 0.4483 - val_loss: 1.1188\n",
      "Epoch 7/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9173 - loss: 0.4591 - val_accuracy: 0.6207 - val_loss: 0.6909\n",
      "Epoch 8/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9313 - loss: 0.3697 - val_accuracy: 0.5517 - val_loss: 1.8095\n",
      "Epoch 9/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9608 - loss: 0.7487 - val_accuracy: 0.5517 - val_loss: 2.9489\n",
      "Epoch 10/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9706 - loss: 0.4706 - val_accuracy: 0.5517 - val_loss: 2.0093\n",
      "Epoch 11/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9944 - loss: 0.1037 - val_accuracy: 0.5517 - val_loss: 1.6261\n",
      "Epoch 12/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9644 - loss: 0.6028 - val_accuracy: 0.6552 - val_loss: 0.7391\n",
      "Epoch 13/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9463 - loss: 0.4808 - val_accuracy: 0.4483 - val_loss: 2.3533\n",
      "Epoch 14/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9825 - loss: 0.1000 - val_accuracy: 0.4483 - val_loss: 3.7736\n",
      "Epoch 15/15\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9817 - loss: 0.2115 - val_accuracy: 0.4483 - val_loss: 4.0941\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=15,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4039406-4036-43f4-acf2-8eb1f2c5c8f2",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e9950f-7e7f-4f71-9b7a-5f5285bc6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,Dense,Flatten, Input, MaxPooling2D, Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Tek satırda resimlerin tamamını okumaya yarıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "310a0e3a-e62b-44c1-b7bf-df77af2fa352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 images belonging to 2 classes.\n",
      "Found 28 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erkan\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 547ms/step - accuracy: 0.4922 - loss: 1.4541 - val_accuracy: 0.5000 - val_loss: 1.3113\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 371ms/step - accuracy: 0.7799 - loss: 0.6093 - val_accuracy: 0.5714 - val_loss: 0.8205\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 341ms/step - accuracy: 0.8646 - loss: 0.2636 - val_accuracy: 0.5357 - val_loss: 1.0707\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 329ms/step - accuracy: 0.8992 - loss: 0.1904 - val_accuracy: 0.6071 - val_loss: 0.7321\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 307ms/step - accuracy: 0.9625 - loss: 0.1294 - val_accuracy: 0.6071 - val_loss: 0.7512\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400ms/step - accuracy: 0.9595 - loss: 0.0880 - val_accuracy: 0.6071 - val_loss: 0.9291\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 379ms/step - accuracy: 0.9748 - loss: 0.0661 - val_accuracy: 0.5714 - val_loss: 1.1027\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 431ms/step - accuracy: 0.9571 - loss: 0.0880 - val_accuracy: 0.6071 - val_loss: 0.9418\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376ms/step - accuracy: 0.9577 - loss: 0.0751 - val_accuracy: 0.6429 - val_loss: 0.8823\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400ms/step - accuracy: 0.9930 - loss: 0.0451 - val_accuracy: 0.5714 - val_loss: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18370a5a4e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir=\"train\"\n",
    "img_width,img_height=70,70\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1/255, validation_split=0.20)\n",
    "\n",
    "train_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_height),\n",
    "                                class_mode=\"binary\", subset=\"training\")\n",
    "\n",
    "test_datagen=ImageDataGenerator( rescale=1/255)\n",
    "test_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_height),\n",
    "                                class_mode=\"binary\", subset=\"validation\")\n",
    "\n",
    "base_model=VGG16(weights=\"imagenet\", input_shape=(img_width,img_height,3),include_top=False)\n",
    "model=Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation=\"relu\")) # Dense nöronları birbirine bağlar\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_datagenerator,epochs=10,validation_data=test_datagenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bfec330-004e-4cad-bcc6-d220a4728c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"clean.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206747f8-ce81-49fd-a7b6-21dec9f97b7e",
   "metadata": {},
   "source": [
    "Sonuc olarak modelimizi hem CNN hem de transfer learning kullanarak egittik. Ancak veri seti oldukca azdi. Bu da modelimizin dogruluk oranini olumsuz yonde etkiledi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f51720-bc86-45f7-aa5e-fdc4ec87200b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
